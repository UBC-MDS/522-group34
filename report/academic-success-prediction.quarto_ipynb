{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Predicting academic success of undergraduate students\"\n",
        "author: \"Catherine Meng, Jenson Chang, Jingyuan Wang and Siddarth Subrahmanian\"\n",
        "date: \"2024/11/23\"\n",
        "embed-resources: true\n",
        "execute:\n",
        "    echo: false\n",
        "format:\n",
        "    html:\n",
        "        toc: true\n",
        "    pdf:\n",
        "        toc: true\n",
        "bibliography: references.bib\n",
        "---"
      ],
      "id": "1e2f750a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "id": "0a308018",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Summary\n",
        "\n",
        "In this analysis, we attempt to build a classification model using the k-nearest neighbors algorithm to predict student dropout and academic success based on information available at enrollment (including academic path, demographics, and socio-economic factors). Our final classifier performed consistently on unseen test data, achieving a cross-validation training score of 0.71, with a similar test score. Although the model's accuracy is moderate, it performs consistently. Given that the data was collected from a single institution, a larger dataset may be necessary to generalize predictions to other institutions or countries. We believe this model is close to supporting dropout prediction for the institution from which the data was collected, though further research to improve performance and better understand characteristics of incorrectly predicted students would still be beneficial.\n",
        "\n",
        "# Introduction\n",
        "\n",
        "Higher education institutions worldwide face the ongoing challenge of academic dropout and student failure, which affect not only individual students’ futures but also the institution’s reputation and resources. The ability to predict and anticipate students' potential difficulties is valuable not only for supporting individual students in achieving their goals but also for institutions aiming to implement strategies that support and guide students who may be at risk of academic failure or dropout.\n",
        "\n",
        "The goal of this analysis is to help reduce academic dropout and failure in higher education by applying machine learning techniques to identify at-risk students early in their academic journey, enabling institutions to implement targeted support strategies.\n",
        "\n",
        "# Methods\n",
        "\n",
        "## Data\n",
        "The data set is created by Mónica Vieira Martins, Jorge Machado, Luís Baptista and Valentim Realinho at the Instituto Politécnico de Portalegre (M.V.Martins, D. Tolledo, J. Machado, L. M.T. Baptista, V.Realinho. @realinho2022predicting). It is sourced from UC Irvine's Machine Learning Repository and can be found [here](https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success). The data contains demographic, enrollment and academic (1st and 2nd semesters) information on the students. Each row in the data set represents a student record. Using these data, a model would be built to predict the academic outcome of the student. There are 36 columns in total.\n",
        "\n",
        "## Analysis\n",
        "\n",
        "The Python programming language @python2021python and the following Python packages were used to perform the analysis: Pandas (@mckinney2011pandas), Scikit-learn (@kramer2016scikit), Pandera (@bantilan2020pandera) and Altair (@vanderplas2018altair). The k-nearest neighbors (k-nn) algorithm was used to build a classification model to predict whether a student is at risk of dropping out. All variables included in the original data set, with the exception of the Course, Nacionality, Gender, Unemployment rate, Inflation rate, GDP, Previous qualification, Mother qualification Mother occupation, Father qualification, Father occupation columns were used to fit the model. Data was split with 80% being partitioned into the training set and 20% being partitioned into the test set. The hyperparameter K was chosen using 5-fold cross validation. All numeric features were standardized just prior to model fitting. We leave the categorical features as they are because they all have integer data type.\n",
        "\n",
        "# Results & Discussion\n",
        "\n",
        "To look at whether each of the predictors might be useful to predict the academic outcome, we plotted the distributions of each predictor from the training data set and coloured the distribution by class (Dropout: blue, Enrolled: orange, and Graduate: red).\n",
        "\n",
        "![Distribution of Numerical Variable per Academic Outcome](../figures/eda_numerical.png){#fig-eda_numerical}\n",
        "\n",
        "In @fig-eda_numerical, although `Unemployment rate`, `Inflation rate` and `GDP` are continous values, they each have less than 10 unique values out of 3000+ rows. This doesn't provide enough range to generalize the problem.\n",
        "\n",
        "![Distribution of Categorical Variable per Academic Outcome](../figures/eda_categorical.png){#fig-eda_categorical}\n",
        "\n",
        "In @fig-eda_categorical, `Previous qualification`, `Mother qualification`, `Mother occupation`, `Father qualification` and `Father occupation` have cluster patterns but it's unclear what the pattern represents since the ranking of education levels are arbitrary. E.g. \"5 - Higher Education - Doctorate\" is ranked higher than \"1 - Secondary Education\" but lower than \"10 - 11th Year of Schooling - Not Completed\". The source data [website](https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success) provides description on each ranking. `Course` only captures 17 different courses and does not generalize the real world well. `Nactionality` and `Gender` are removed to avoid racial and gender bias\n"
      ],
      "id": "0a835a0b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_df = pd.read_csv('./../data/processed/train_data.csv')\n",
        "test_df = pd.read_csv('./../data/processed/test_data.csv')\n",
        "\n",
        "X_train = train_df.drop(columns=['Target'])\n",
        "y_train = train_df['Target']\n",
        "X_test = test_df.drop(columns=['Target'])\n",
        "y_test = test_df['Target']\n",
        "\n",
        "model_path = './../models/best_knn_pipeline.pickle'\n",
        "with open(model_path, 'rb') as f:\n",
        "    best_model = pickle.load(f)\n",
        "\n",
        "test_score = best_model.score(X_test, y_test)"
      ],
      "id": "88a2a728",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We utilized the KNN to train the dataset and employed RandomizedSearchCV to fine-tune the hyperparameters. Based on the results, the optimal hyperparameter value is k=12, achieving a best cross-validation score of 0.71. Using this value, we retrained the model and evaluated its performance on the test set, obtaining a final test score of 0.71.\n"
      ],
      "id": "e60fd0cb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y_pred = best_model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "id": "30179fb1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "{{< pagebreak >}}\n",
        "\n",
        "\n",
        "## References\n"
      ],
      "id": "3991fb5f"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}